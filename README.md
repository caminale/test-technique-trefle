# Trefle test technique


## Pr√©requis :

+ Airflow : https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html
+ Python 3.10
+ Snowflake Free Trial : https://signup.snowflake.com/ 
+ Docker

## Configuration

1. Il faut construire une image Docker Airflow avec les d√©pendances python n√©cessaire :   
`make construire-image-airflow`
2. Lancer le docker compose, cette commande permettra de configurer et lancer Airflow :  
`make lancer-airflow`
3. Configurer les connexions sur Airflow : 
    + Cr√©er une connexion avec aws :   
   ![img.png](static/img.png)
    + Cr√©er une connexion avec snowflake :   
   ![img_1.png](static/img_1.png)
4. Lancer la DAG : `ingestion_donnees_plantes_dans_snowflake`  
   ![img_2.png](static/img_2.png)

## Workflow du DAG :

#### La premi√®re √©tape 

La **premi√®re √©tape consiste** √† r√©cup√©rer les donn√©es de plantes sur l'API Trefle.  
> On pourra retrouver le code concern√© dans le dossier [src](./src), le code respecte plus ou moins `la clean archi`.

Cette √©tape r√©cup√®re par pagination des listes de donn√©es et pour chaque √©lement, 
nous allons le tranformer en objet python Plante qui respectera un schema impos√©. Cet objets python sera ensuite ajouter
au dataframe pandas. Au bout de 500 pages, nous enregistrons ce dataframe sous format 
parquet sur le stockage objet d'aws.   

![Stockage sur AWS](static/img_3.png)

#### La deuxi√®me √©tape :

**Pr√©requis :**
+ Cr√©er une database sur la console web de snowflake üñê

La **deuxi√®me √©tape consiste** √† configurer Snowflake pour recevoir les donn√©es de plantes d'aws.
> Le code est principalement dans le fichier [creation_table_plante](dags/scripts-sql/creation_table_plante.sql)  

Ce code (sql) va permettre de cr√©er : 
+ Un format de fichier pour le stage ici : `parquet`
+ Un stage avec les informations du bucket s3 qui sera utilis√© pour la 3√®me √©tape.
+ Une table Plante avec un schema qui correspond √† celui de nos parquets

#### La troisi√®me √©tape :

La **troisi√®me √©tape consiste** √† charger les donn√©es de s3 vers Snowflake. 
> Le code est dans le fichier [copier_donnees_s3_vers_snowflake](dags/scripts-sql/copier_donnees_s3_vers_snowflake.sql)

Le code (sql) permet de r√©cuperer toutes les donn√©es dans les fichiers `.parquet` et les charger dans la table snowflake
`plante`.

![img.png](static/snowflake_cols.png)

![img_1.png](static/snowflake_data_preview.png)
#### Aller plus loin : 

+ G√©rer les cr√©dentials en local et sur nos environnements iso(prod) avec parameter store et en local avec un ansible vault : 
  + Pour le token de l'api 
  + Aws cr√©dentials
+ D√©ployer le docker compose dans une machine Ec2 (avec terraform)
+ Automatiser la cr√©ation de connexion Airflow pour snowflake ([terraform](https://registry.terraform.io/providers/DrFaust92/airflow/latest/docs/resources/airflow_connection))
+ Automatiser la cr√©ation de la database snowflake ([terraform](https://registry.terraform.io/providers/Snowflake-Labs/snowflake/latest/docs/resources/database))

#### Utiles : üí°

+ R√©cup√©rer les infos sur le hostname :SELECT SYSTEM$ALLOWLIST()
+ Tester la connexion en local avec `snowcd`
+ Cr√©ation stage doc snowflake : https://docs.snowflake.com/fr/user-guide/data-load-s3-create-stage.html
+ S3Hook pour r√©cup√©rer les cr√©dentials aws sur airflow pour les donner au code : https://hevodata.com/learn/airflow-s3-hook/